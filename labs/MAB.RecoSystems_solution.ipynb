{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAB.RecoSystems_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiHHQy67xvRX",
        "colab_type": "text"
      },
      "source": [
        "# Bandits for Recommendation - RLSS 2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9ah6wKlxvRj",
        "colab_type": "text"
      },
      "source": [
        "For conveniency slides used for presentation are available on https://www.dropbox.com/s/6px7a37qddstgtn/RLSS.pdf?dl=0\n",
        "\n",
        "The objective of this notebook is to apply the bandits algorithms to recommendation problem using a simulated envrionment. Although in practice you would also use the real data, the complexity of the recommendation problem and the associated algorithmic challenges can already be revealed even in this simple setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0qypKtkxvRm",
        "colab_type": "text"
      },
      "source": [
        "![RecSys](https://github.com/yfletberliac/rlss2019-hands-on/blob/master/imgs/recsys_scheme.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUHt3wDixvRp",
        "colab_type": "text"
      },
      "source": [
        "## Controlled recommendation environment\n",
        "\n",
        "In the simulated environment, user browsers a web-site and might click on recommendation served by a recommendation agent. The goal of the agent is to maximize the number of clicks. The simulation is going to be a little bit more involved that ideal situation from the previous TP. In particular we are going to work on a stream of user which are also generating some \"organic\" observations, meaning that you collect some browsing events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUgV9SHWxvRu",
        "colab_type": "text"
      },
      "source": [
        "### Define action, observation, reward\n",
        "\n",
        "* Action -- a recommended item (e.g., in e-commerce it can be a product)\n",
        "* Reward -- user interaction with the recommendation (e.g., a click)\n",
        "* Observation -- user activity (e.g., list of products that user has visited during his/her browsing session). Here we report only \"organic\" event which are not the recommended item and can occur even if the recommendation was not used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e02bLEozxvRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you are running this notebook on Colab, setup with the following:\n",
        "!git clone https://github.com/yfletberliac/rlss2019-hands-on.git > /dev/null 2>&1\n",
        "import sys\n",
        "sys.path.insert(0, './rlss2019-hands-on/utils/rec_systems')\n",
        "# If using the notebook locally, replace by:\n",
        "# sys.path.insert(0, '../utils/rec_systems')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N2hBY94pxvR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from reco_env import RecoEnv, env_1_args\n",
        "from configuration import Configuration\n",
        "from agent import Agent, RandomAgent, random_args\n",
        "\n",
        "# you can overwrite environment arguments here\n",
        "RND_SEED = 1234\n",
        "env_1_args['random_seed'] = RND_SEED\n",
        "\n",
        "# create environment with configuration\n",
        "env = RecoEnv(Configuration({\n",
        "            **env_1_args,\n",
        "}))\n",
        "env.reset()\n",
        "\n",
        "# random agent\n",
        "rand_agent = RandomAgent(Configuration({\n",
        "    **random_args,    \n",
        "}))\n",
        "\n",
        "# counting steps\n",
        "i = 0\n",
        "\n",
        "observation, _, _, _ = env.step(None)\n",
        "reward, done = 0, False\n",
        "while not done:\n",
        "    # choose action given current observation\n",
        "    action = rand_agent.act(observation)            \n",
        "    # execute action in the environment\n",
        "    observation, reward, done, info = env.step(action['a'])\n",
        "    print(f\"Step: {i} - Action: {action} - Observation: {observation} - Reward: {reward}\")\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qrBAbF7xvR-",
        "colab_type": "text"
      },
      "source": [
        "### Simulation of user response to recommendation\n",
        "\n",
        "User response to recommendation is modeled as a function of (1) affinity of user to recommended product, and (2) correction due to the recommendation. \n",
        "\n",
        "$\\mu(u,p,t) := f(\\Lambda(u,p,t) + \\epsilon(u,p,t))$,\n",
        "\n",
        "where $f$ is an increasing function, $\\Lambda(u,p,t)$ is the log odds of user $u$ being interested by product $a$ at time $t$, $\\epsilon(u,p,t))$ is a zero mean correction.\n",
        "\n",
        "Assuming the latent space for user and product, let $\\omega \\in \\mathbb{R}^K$ be the latent representation of user $u$ of size $K$, $\\beta \\in \\mathbb{R}^K$ is the latent reprentation of product $p$, then user response on recommendation can be modeled as\n",
        "\n",
        "$\\mu(u,p,t) := \\text{sigmoid}(\\beta^T \\omega + \\mu_\\epsilon)$, \n",
        "\n",
        "where $\\omega_i = \\mathcal{N}(0, \\sigma^2_\\omega)$, $\\beta_i = \\mathcal{N}(0, 1)$, $\\mu_\\epsilon = \\mathcal{N}(0, \\sigma^2_\\mu)$.\n",
        "\n",
        "In advertisement, typical values for $\\mu(u,p,t)$ are around $0.02$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvONEX7hxvSA",
        "colab_type": "text"
      },
      "source": [
        "## Online learning using recommendation environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF-sScslxvSC",
        "colab_type": "text"
      },
      "source": [
        "We introduce functions that train, evaluate and plot the evaluation metrics of recommendation agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFcJAtJxvSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from train_eval_utils import train_eval_agents, plot_ctr\n",
        "\n",
        "help(train_eval_agents)\n",
        "help(plot_ctr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPt5OKQNxvSN",
        "colab_type": "text"
      },
      "source": [
        "## Bandits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiD0TEj_xvSP",
        "colab_type": "text"
      },
      "source": [
        "Code bandits algorithms that you have already seen in the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483xV9EKxvSR",
        "colab_type": "text"
      },
      "source": [
        "### UCB algorithm [Auer 2002]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yArPY2b_xvST",
        "colab_type": "text"
      },
      "source": [
        "Code the agent that runs the UCB algorithm for product click-through rate (number of clicks / number of displays)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4N-yK2TxvSU",
        "colab_type": "text"
      },
      "source": [
        "* First, code the bound based on Hoeffding inequality\n",
        "\n",
        "$I_k = argmax_k \\hat{\\mu}_{k,n} + \\sqrt{\\frac{2 \\log t}{n}}$\n",
        "\n",
        "\n",
        "* Then, improve by tuning $\\alpha$ \n",
        "\n",
        "$I_k = argmax_k \\hat{\\mu}_{k,n} + \\alpha \\sqrt{\\frac{2 \\log t}{n}}$\n",
        "\n",
        "* Finally, use the fact that click is a Bernoulli random variable to obtain the exact bound"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmiy7clOxvSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Implement an Agent interface\n",
        "class AlphaUCBAgent(Agent):\n",
        "    def __init__(self, config):\n",
        "        super(AlphaUCBAgent, self).__init__(config)\n",
        "\n",
        "        # Init with ones to avoid division by zero\n",
        "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)\n",
        "        self.product_counts = np.ones(self.config.num_products, dtype=np.float32)       \n",
        "        # alpha parameter\n",
        "        self.alpha = 0.01        \n",
        "        \n",
        "    def train(self, observation, action, reward, done):\n",
        "        \"\"\"Train from observed data\"\"\"\n",
        "\n",
        "        if reward is not None and action is not None:\n",
        "            self.product_rewards[action] += reward\n",
        "            self.product_counts[action] += 1\n",
        "\n",
        "    def act(self, observation):\n",
        "        \"\"\"Return an action given current observation\"\"\"\n",
        "        \n",
        "        t = sum(self.product_counts)\n",
        "        ucb = self.product_rewards / self.product_counts + self.alpha * np.sqrt(2.0*np.log(t)/self.product_counts)\n",
        "        action = np.argmax(ucb)\n",
        "\n",
        "        return { 'a': action }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwH44jHjxvSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats.distributions import beta\n",
        "\n",
        "def beta_ucb(num_clicks, num_displays):\n",
        "    return beta.ppf(0.975, num_clicks + 1, num_displays - num_clicks+ 1)\n",
        "\n",
        "class BetaUCBAgent(Agent):\n",
        "    def __init__(self, config):\n",
        "        super(BetaUCBAgent, self).__init__(config)\n",
        "\n",
        "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)\n",
        "        self.product_counts = np.ones(self.config.num_products, dtype=np.float32)\n",
        "        \n",
        "        self.beta_ucb_func = np.vectorize(beta_ucb)\n",
        "        \n",
        "    def train(self, observation, action, reward, done):\n",
        "        if reward is not None and action is not None:\n",
        "            self.product_rewards[action] += reward\n",
        "            self.product_counts[action] += 1\n",
        "\n",
        "    def act(self, observation):\n",
        "        ucb = self.beta_ucb_func(self.product_rewards, self.product_counts)\n",
        "        action = np.argmax(ucb)\n",
        "\n",
        "        return { 'a': action }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMKqe0l5xvSl",
        "colab_type": "text"
      },
      "source": [
        "### Compare UCB agents performance and running time in stochastic bandits setting\n",
        "\n",
        "* Train and evaluate UCB with Hoeffding bound and exact bound\n",
        "\n",
        "* Achieve similar performance by tuning $\\alpha$ parameter\n",
        "\n",
        "* Compare the running time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_DnA7R9xvSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of products to recommend\n",
        "num_products = 10\n",
        "# number of users for train and evaluation\n",
        "num_train_users, num_eval_users = 1000, 1000\n",
        "\n",
        "custom_args = { 'num_products': num_products,\n",
        "                'random_seed': RND_SEED,\n",
        "              }\n",
        "config = Configuration({ \n",
        "        **env_1_args,\n",
        "        **custom_args,\n",
        "})\n",
        "\n",
        "alpha_ucb_agent = AlphaUCBAgent(config)\n",
        "beta_ucb_agent = BetaUCBAgent(config)\n",
        "rand_agent = RandomAgent(config)\n",
        "\n",
        "agents = [rand_agent, alpha_ucb_agent, beta_ucb_agent]\n",
        "\n",
        "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
        "print(stats)\n",
        "\n",
        "plot_ctr(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjBGBp3PxvSv",
        "colab_type": "text"
      },
      "source": [
        "### Exp3 / Boltzmann exploration algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3mNTqIxvSz",
        "colab_type": "text"
      },
      "source": [
        "* Code the agent that runs the Exp3 / Boltzmann exploration algorithm\n",
        "\n",
        "* Tune temperature parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8Bp5nHxvS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import logsumexp\n",
        "from numpy.random import choice\n",
        "\n",
        "class Exp3Agent(Agent):\n",
        "    def __init__(self, config):\n",
        "        super(Exp3Agent, self).__init__(config)\n",
        "\n",
        "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)    \n",
        "        # softmax temperature\n",
        "        self.eta = 1\n",
        "        \n",
        "    def train(self, observation, action, reward, done):\n",
        "        if reward is not None and action is not None:\n",
        "            self.product_rewards[action] += reward / self.probs()[action] \n",
        "            \n",
        "    def log_softmax(self, vec):\n",
        "        return vec - logsumexp(vec)\n",
        "\n",
        "    def softmax(self, vec):\n",
        "        probs = np.exp(self.log_softmax(vec))\n",
        "        probs /= probs.sum()\n",
        "        return probs\n",
        "      \n",
        "    def probs(self):\n",
        "      return self.softmax(self.eta * self.product_rewards)\n",
        "    \n",
        "    def act(self, observation):\n",
        "        prob = self.probs()\n",
        "        action = choice(self.config.num_products, p = prob)\n",
        "\n",
        "        return { 'a': action, 'ps': prob[action] }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPtECIK7xvS8",
        "colab_type": "text"
      },
      "source": [
        "### Compare UCB and Exp3 agents performance and running time in stochastic bandits setting\n",
        "\n",
        "* Train and evaluate UCB and Exp3 algorithms against random product recommendation\n",
        "\n",
        "* Increase the number of products to 100 and explain the change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9qc5zixvS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of products to recommend\n",
        "num_products = 10\n",
        "# number of users for train and evaluation\n",
        "num_train_users, num_eval_users = 1000, 1000\n",
        "\n",
        "custom_args = { 'num_products': num_products,\n",
        "                'random_seed': RND_SEED,\n",
        "              }\n",
        "config = Configuration({ \n",
        "        **env_1_args,\n",
        "        **custom_args,\n",
        "})\n",
        "\n",
        "ucb_agent = AlphaUCBAgent(config)\n",
        "exp3_agent = Exp3Agent(config)\n",
        "rand_agent = RandomAgent(config)\n",
        "\n",
        "agents = [rand_agent, ucb_agent, exp3_agent]\n",
        "\n",
        "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
        "print(stats)\n",
        "\n",
        "plot_ctr(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4j5e5NZ3KLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of products to recommend\n",
        "num_products = 100\n",
        "# number of users for train and evaluation\n",
        "num_train_users, num_eval_users = 1000, 1000\n",
        "\n",
        "custom_args = { 'num_products': num_products,\n",
        "                'random_seed': RND_SEED,\n",
        "              }\n",
        "config = Configuration({ \n",
        "        **env_1_args,\n",
        "        **custom_args,\n",
        "})\n",
        "\n",
        "ucb_agent = AlphaUCBAgent(config)\n",
        "exp3_agent = Exp3Agent(config)\n",
        "rand_agent = RandomAgent(config)\n",
        "\n",
        "agents = [rand_agent, ucb_agent, exp3_agent]\n",
        "\n",
        "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
        "print(stats)\n",
        "\n",
        "plot_ctr(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xezerwyyxvTE",
        "colab_type": "text"
      },
      "source": [
        "### Compare UCB and Exp3 agents in adversarial setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FbkJSExvTG",
        "colab_type": "text"
      },
      "source": [
        "Modeling the user state change during time leads to non-stationary user response to a recommendation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWe3cD8TxvTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_products = 100\n",
        "num_train_users, num_eval_users = 1000, 1000\n",
        "\n",
        "# adversarial setting: increase user state change during time\n",
        "custom_args = { 'num_products': num_products,\n",
        "                'random_seed': RND_SEED,                \n",
        "                'sigma_omega': 0.3\n",
        "              }\n",
        "config = Configuration({ \n",
        "        **env_1_args,\n",
        "        **custom_args,\n",
        "})\n",
        "\n",
        "ucb_agent = AlphaUCBAgent(config)\n",
        "exp3_agent = Exp3Agent(config)\n",
        "rand_agent = RandomAgent(config)\n",
        "\n",
        "agents = [rand_agent, ucb_agent, exp3_agent]\n",
        "\n",
        "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
        "print(stats)\n",
        "\n",
        "plot_ctr(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mLzLMuxvTO",
        "colab_type": "text"
      },
      "source": [
        "## Compare sample complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evPgWxoGxvTQ",
        "colab_type": "text"
      },
      "source": [
        "$\\sigma$-subgaussian distribution\n",
        "* UCB\n",
        "$R_T = \\mathcal{O}(\\sum_{i>1} \\frac{\\log T}{\\Delta_i})$\n",
        "* Exp3\n",
        "$R_T = \\mathcal{O}(\\sum_{i>1}\\frac{\\log^2 (T \\Delta_i^2)}{\\Delta_i})$\n",
        "\n",
        "distribution-independent\n",
        "* UCB\n",
        "$R_T = \\mathcal{O}(\\sqrt{KT\\log T})$\n",
        "* Exp3\n",
        "$R_T = \\mathcal{O}(\\sqrt{KT}\\log K)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B-HsCB0xvTV",
        "colab_type": "text"
      },
      "source": [
        "## Using side data: browsing events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ3et157xvTY",
        "colab_type": "text"
      },
      "source": [
        "To make algorithms more sample efficient, we will use side data to bootstrap the recommendation. Specifically, we will use the user browsing events (aka \"organic\" events). The amount of browsing data is typically much larger than the number of click events on recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0av0mPaxvTZ",
        "colab_type": "text"
      },
      "source": [
        "### Popularity agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiYsjPbHxvTb",
        "colab_type": "text"
      },
      "source": [
        "The simpliest recommedation agent is based on the total number of views of the product during browsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L__UBnUbxvTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PopularityAgent(Agent):\n",
        "    def __init__(self, config):\n",
        "        super(PopularityAgent, self).__init__(config)\n",
        "\n",
        "        # Track number of times each item is viewed in organic session\n",
        "        self.organic_views = np.ones(self.config.num_products)\n",
        "\n",
        "    def train(self, observation, action, reward, done):\n",
        "        # Increment organic view counts\n",
        "        if observation:\n",
        "            for view in observation:\n",
        "                self.organic_views[view] += 1\n",
        "\n",
        "    def act(self, observation):\n",
        "        # Choosing action proportionaly to the number of views\n",
        "        prob = self.organic_views / sum(self.organic_views)\n",
        "        action = choice(self.config.num_products, p = prob)\n",
        "        \n",
        "        return { 'a': action, 'ps': prob[action] }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBA57mwGxvTi",
        "colab_type": "text"
      },
      "source": [
        "### Contextual Bandit\n",
        "\n",
        "Improve the popularity agent by personalizing popularity to the user interest. We represent the user interest by the last product he/she has seen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5UmulQoxvTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import logsumexp\n",
        "\n",
        "class ContextualExp3Agent(Agent):\n",
        "    def __init__(self, config):\n",
        "        super(ContextualExp3Agent, self).__init__(config)\n",
        "\n",
        "        self.product_rewards = np.zeros((self.config.num_products, self.config.num_products))\n",
        "        self.last_product_seen = None\n",
        "        # softmax temperature parameter\n",
        "        self.eta = 0.01\n",
        "                \n",
        "    def update_lpv(self, observation):\n",
        "        \"\"\"Updates the last product viewed based on the observation\"\"\"\n",
        "        if observation:\n",
        "            self.last_product_seen = observation[-1]\n",
        "            \n",
        "    def train(self, observation, action, reward, done):\n",
        "        if observation:\n",
        "            for view1 in observation:\n",
        "                for view2 in observation:\n",
        "                    if view1 != view2:\n",
        "                        self.product_rewards[view1, view2] += 1\n",
        "            \n",
        "    def log_softmax(self, vec):\n",
        "        return vec - logsumexp(vec)\n",
        "\n",
        "    def softmax(self, vec):\n",
        "        probs = np.exp(self.log_softmax(vec))\n",
        "        probs /= probs.sum()\n",
        "        return probs\n",
        "    \n",
        "    def act(self, observation):\n",
        "        self.update_lpv(observation)\n",
        "        \n",
        "        prob = self.softmax(self.eta * self.product_rewards[self.last_product_seen, :])\n",
        "        action = choice(self.config.num_products, p = prob)\n",
        "\n",
        "        return { 'a': action, 'ps': prob[action] }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAXOmt-rxvTr",
        "colab_type": "text"
      },
      "source": [
        "### Compare agents that use click events to agents that use view events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCvSA_EVxvTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_products = 100\n",
        "num_train_users, num_eval_users = 1000, 1000\n",
        "\n",
        "# increase difference among users\n",
        "custom_args = { 'num_products': num_products,\n",
        "                'random_seed': RND_SEED,\n",
        "                'sigma_omega_initial': 2.0,                \n",
        "              }\n",
        "config = Configuration({ \n",
        "        **env_1_args,\n",
        "        **custom_args,\n",
        "})\n",
        "\n",
        "contextual_exp3_agent = ContextualExp3Agent(config)\n",
        "exp3_agent = Exp3Agent(config)\n",
        "pop_agent = PopularityAgent(config)\n",
        "ucb_agent = AlphaUCBAgent(config)\n",
        "rand_agent = RandomAgent(config)\n",
        "\n",
        "agents = [pop_agent, contextual_exp3_agent, exp3_agent, ucb_agent, rand_agent]\n",
        "\n",
        "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
        "print(stats)\n",
        "\n",
        "plot_ctr(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnCMvINKxvT0",
        "colab_type": "text"
      },
      "source": [
        "## Offline evaluation\n",
        "\n",
        "Running counterfactual evaluations on real datasets use more time than a reasonnable practical sessions does. \n",
        "We are going to exhibit the core of the problems theses algorithms faces on a very toy example. \n",
        "\n",
        "Assume we are running a $pi_0$ policy random uniform on the possible sets of arms of size 20 with a number of context 10. This means that at each timestep we have 200 possibles combinations to create the recommendation which is very small. Moreover we assume that the obtained reward is a constant equal to 1. This means we have no stochasticity due to realization of the sample and any policy $\\pi$ should have the same estimator of averaged reward.\n",
        "\n",
        "1. We assume the new $\\pi$ is always choosing arm 1, plot the estimation of the IPS  with respect to the number of samples on one run. If you want to be close to the reality (and do more sophisticated experiments), it is better to simulate a dataset $S=\\{context_t, action_t, propensity_t, reward_t\\}$ and then write the IPS estimation rather than rely on mathematical properties of our minimalist system. Also remark than in real system this is a good idea to also store the possible set of actions at each timestep. What do you think of this estimator while you can sample $S$ only one time ? \n",
        "\n",
        "2. Supperpose the self normalized operator on the same plot for each time step. Keep in mind this is actually the best possible case for SNIPS and that for more complex scenarios many others problems can be more important than the variance fixed by snips. \n",
        "\n",
        "3. Now the reward is 1 with probability 1/2 for arm number 2 and 1 with probability 1/3 for all other arms. The $\\pi_0$ policy select arm one with probability 0.9 and an arm at random with proba 0.1. Build estimates of the policy selecting always arm 2. How many samples do you need to identify with confidence that this policy is better than the logging policy ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jAdiGlixvT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def create_dummy_dataset(nbcontext,nbactions, nbsamples):\n",
        "    contexts = np.random.randint(0, nbcontext, (nbsamples, 1) )\n",
        "    actions =  np.random.randint(0, nbactions, (nbsamples, 1) )\n",
        "    propensities = np.ones( (nbsamples, 1) )/nbactions\n",
        "    rewards = np.ones( (nbsamples, 1) )\n",
        "    return np.concatenate( (contexts, actions, propensities, rewards), axis=1 )\n",
        "S = create_dummy_dataset(10, 20, 1000)\n",
        "\n",
        "def always_one_policy(context, possible_actions):\n",
        "    probas = np.zeros(possible_actions.shape)\n",
        "    probas[0] = 1\n",
        "    return probas\n",
        "\n",
        "def evaluate_IPS(S, policy):\n",
        "    #Explicit loop for clarity, this should be done with a \"apply_along_axis\"\n",
        "    g = 0\n",
        "    ans = np.zeros(S.shape[0]) * np.nan\n",
        "    possible_actions = np.arange(S[:,1].max()+1)\n",
        "    for t in range(S.shape[0]):\n",
        "        actions_probas = policy(S[t,:], possible_actions)\n",
        "        logged_action = int(S[t,1])\n",
        "        prop =  S[t,2]\n",
        "        delta = S[t,3]\n",
        "        g += delta * actions_probas[logged_action] / prop\n",
        "        ans[t] = g/(t+1) \n",
        "    return ans \n",
        "\n",
        "def evaluate_SNIPS(S, policy):\n",
        "    num = 0\n",
        "    denom = 0\n",
        "    ans = np.zeros(S.shape[0]) * np.nan\n",
        "    possible_actions = np.arange(S[:,1].max()+1)\n",
        "    for t in range(S.shape[0]):\n",
        "        actions_probas = policy(S[t,:], possible_actions)\n",
        "        logged_action = int(S[t,1])\n",
        "        prop =  S[t,2]\n",
        "        delta = S[t,3]\n",
        "        num += (delta * actions_probas[logged_action] / prop) \n",
        "        denom += (actions_probas[logged_action] / prop) \n",
        "        if denom>0: ans[t] = num/denom\n",
        "    return ans \n",
        "\n",
        "ips = evaluate_IPS(S, always_one_policy)\n",
        "snips = evaluate_SNIPS(S, always_one_policy)\n",
        "plt.plot(ips, 'b')\n",
        "plt.plot(snips, 'r')\n",
        "plt.legend( ('ips', 'snips') )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnMVU7VxvT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHrmoKJqxvUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri_qiMjgxvUO",
        "colab_type": "text"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwEO0q4xvUQ",
        "colab_type": "text"
      },
      "source": [
        "* RecoGym https://github.com/criteo-research/reco-gym\n",
        "* Bandit Algorithms, 2018 by Tor Lattimore and Csaba Szepesv√°ri"
      ]
    }
  ]
}