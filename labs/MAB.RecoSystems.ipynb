{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandits for Recommendation - RLSS 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For conveniency slides used for presentation are available on https://www.dropbox.com/s/6px7a37qddstgtn/RLSS.pdf?dl=0\n",
    "\n",
    "The objective of this notebook is to apply the bandits algorithms to recommendation problem using a simulated envrionment. Although in practice you would also use the real data, the complexity of the recommendation problem and the associated algorithmic challenges can already be revealed even in this simple setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RecSys](https://github.com/yfletberliac/rlss2019-hands-on/blob/master/imgs/recsys_scheme.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled recommendation environment\n",
    "\n",
    "In the simulated environment, user browsers a web-site and might click on recommendation served by a recommendation agent. The goal of the agent is to maximize the number of clicks. The simulation is going to be a little bit more involved that ideal situation from the previous TP. In particular we are going to work on a stream of user which are also generating some \"organic\" observations, meaning that you collect some browsing events and have to perform some recommendations until the user decides to leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define action, observation, reward\n",
    "\n",
    "* Action -- a recommended item (e.g., in e-commerce it can be a product). Here the simulator will be set to have 100 possible products\n",
    "* Reward -- user interaction with the recommendation (e.g., a click)\n",
    "* Observation -- user activity (e.g., list of products that user has visited during his/her browsing session). Here we report only \"organic\" event which are not the recommended item and can occur even if the recommendation was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook on Colab, setup with the following:\n",
    "!git clone https://github.com/yfletberliac/rlss2019-hands-on.git > /dev/null 2>&1\n",
    "import sys\n",
    "sys.path.insert(0, './rlss2019-hands-on/utils/rec_systems')\n",
    "# If using the notebook locally, replace by:\n",
    "# sys.path.insert(0, '../utils/rec_systems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco_env import RecoEnv, env_1_args\n",
    "from configuration import Configuration\n",
    "from agent import Agent, RandomAgent, random_args\n",
    "\n",
    "# you can overwrite environment arguments here\n",
    "RND_SEED = 1234\n",
    "env_1_args['random_seed'] = RND_SEED\n",
    "\n",
    "# create environment with configuration\n",
    "env = RecoEnv(Configuration({\n",
    "            **env_1_args,\n",
    "}))\n",
    "env.reset()\n",
    "\n",
    "# random agent\n",
    "rand_agent = RandomAgent(Configuration({\n",
    "    **random_args,    \n",
    "}))\n",
    "\n",
    "# counting steps\n",
    "i = 0\n",
    "\n",
    "observation, _, _, _ = env.step(None)\n",
    "reward, done = 0, False\n",
    "while not done:\n",
    "    # choose action given current observation\n",
    "    action = rand_agent.act(observation)            \n",
    "    # execute action in the environment\n",
    "    observation, reward, done, info = env.step(action['a'])\n",
    "    print(f\"Step: {i} - Action: {action} - Observation: {observation} - Reward: {reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of user response to recommendation\n",
    "\n",
    "User response to recommendation is modeled as a function of (1) affinity of user to recommended product, and (2) correction due to the recommendation. \n",
    "\n",
    "$\\mu(u,p,t) := f(\\Lambda(u,p,t) + \\epsilon(u,p,t))$,\n",
    "\n",
    "where $f$ is an increasing function, $\\Lambda(u,p,t)$ is the log odds of user $u$ being interested by product $a$ at time $t$, $\\epsilon(u,p,t))$ is a zero mean correction.\n",
    "\n",
    "Assuming the latent space for user and product, let $\\omega \\in \\mathbb{R}^K$ be the latent representation of user $u$ of size $K$, $\\beta \\in \\mathbb{R}^K$ is the latent reprentation of product $p$, then user response on recommendation can be modeled as\n",
    "\n",
    "$\\mu(u,p,t) := \\text{sigmoid}(\\beta^T \\omega + \\mu_\\epsilon)$, \n",
    "\n",
    "where $\\omega_i = \\mathcal{N}(0, \\sigma^2_\\omega)$, $\\beta_i = \\mathcal{N}(0, 1)$, $\\mu_\\epsilon = \\mathcal{N}(0, \\sigma^2_\\mu)$.\n",
    "\n",
    "In advertisement, typical values for $\\mu(u,p,t)$ are around $0.02$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning using recommendation environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce functions that train, evaluate and plot the evaluation metrics of recommendation agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_eval_utils import train_eval_agents, plot_ctr\n",
    "\n",
    "help(train_eval_agents)\n",
    "help(plot_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code bandits algorithms that you have already seen in the class. In this part the simulator is configured in a such way that we are actually facing the stochastic bandit setting: all the users are the same and their preferences are not evolving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB algorithm [Auer 2002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code the agent that runs the UCB algorithm for product click-through rate (number of clicks / number of displays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we provide the code for bound based on Hoeffding inequality\n",
    "\n",
    "$I_k = argmax_k \\hat{\\mu}_{k,n} + \\sqrt{\\frac{2 \\log t}{n}}$\n",
    "\n",
    "\n",
    "* Then, improve by tuning $\\alpha$ \n",
    "\n",
    "$I_k = argmax_k \\hat{\\mu}_{k,n} + \\alpha \\sqrt{\\frac{2 \\log t}{n}}$\n",
    "\n",
    "* Finally, use the fact that click is a Bernoulli random variable to obtain a sharper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Implement an Agent interface\n",
    "class AlphaUCBAgent(Agent):\n",
    "    def __init__(self, config):\n",
    "        super(AlphaUCBAgent, self).__init__(config)\n",
    "\n",
    "        # Init with ones to avoid division by zero\n",
    "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)\n",
    "        self.product_counts = np.ones(self.config.num_products, dtype=np.float32)       \n",
    "        # alpha parameter (already tuned)\n",
    "        self.alpha = 0.01        \n",
    "        \n",
    "    def train(self, observation, action, reward, done):\n",
    "        \"\"\"Train from observed data\"\"\"\n",
    "\n",
    "        if reward is not None and action is not None:\n",
    "            self.product_rewards[action] += reward\n",
    "            self.product_counts[action] += 1\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"Return an action given current observation\"\"\"\n",
    "        \n",
    "        t = sum(self.product_counts)\n",
    "        ucb = self.product_rewards / t + self.alpha * np.sqrt(2.0*np.log(t)/self.product_counts)\n",
    "        action = np.argmax(ucb)\n",
    "\n",
    "        return { 'a': action }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: code the beta bound\n",
    "def ucb_bound(num_clicks, num_displays):\n",
    "    return ..\n",
    "\n",
    "class BetaUCBAgent(Agent):\n",
    "    def __init__(self, config):\n",
    "        super(BetaUCBAgent, self).__init__(config)\n",
    "\n",
    "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)\n",
    "        self.product_counts = np.ones(self.config.num_products, dtype=np.float32)\n",
    "        \n",
    "        self.ucb_func = np.vectorize(ucb_bound)\n",
    "        \n",
    "    def train(self, observation, action, reward, done):\n",
    "        if reward is not None and action is not None:\n",
    "            self.product_rewards[action] += reward\n",
    "            self.product_counts[action] += 1\n",
    "\n",
    "    def act(self, observation):\n",
    "        ucb = self.ucb_func(self.product_rewards, self.product_counts)\n",
    "        action = np.argmax(ucb)\n",
    "\n",
    "        return { 'a': action }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare UCB agents performance and running time in stochastic bandits setting\n",
    "\n",
    "* Train and evaluate UCB with Hoeffding bound and exact bound\n",
    "\n",
    "* Achieve similar performance by tuning $\\alpha$ parameter\n",
    "\n",
    "* Compare the running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of products to recommend\n",
    "num_products = 10\n",
    "# number of users for train and evaluation\n",
    "num_train_users, num_eval_users = 1000, 1000\n",
    "\n",
    "custom_args = { 'num_products': num_products,\n",
    "                'random_seed': RND_SEED,\n",
    "              }\n",
    "config = Configuration({ \n",
    "        **env_1_args,\n",
    "        **custom_args,\n",
    "})\n",
    "\n",
    "alpha_ucb_agent = AlphaUCBAgent(config)\n",
    "beta_ucb_agent = BetaUCBAgent(config)\n",
    "rand_agent = RandomAgent(config)\n",
    "\n",
    "agents = [rand_agent, alpha_ucb_agent, beta_ucb_agent]\n",
    "\n",
    "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
    "print(stats)\n",
    "\n",
    "plot_ctr(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp3 / Boltzmann exploration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code the agent that runs the Exp3 / Boltzmann exploration algorithm. The adversarial setting is going to be described later in the course but you can find it [Tor Lattimore's bandits blog](https://banditalgs.com/2016/10/01/adversarial-bandits/): [43].\n",
    "__Remark:__ *it is possible to have a exponential speedup of the sampling unsing storing the weights in a binary tree containing partial sums, see http://timvieira.github.io/blog/post/2016/11/21/heaps-for-incremental-computation/*\n",
    "\n",
    "* Tune temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "from numpy.random import choice\n",
    "\n",
    "class Exp3Agent(Agent):\n",
    "    def __init__(self, config):\n",
    "        super(Exp3Agent, self).__init__(config)\n",
    "\n",
    "        self.product_rewards = np.zeros(self.config.num_products, dtype=np.float32)    \n",
    "        self.product_counts = np.ones(self.config.num_products, dtype=np.float32)\n",
    "        # TODO: tune softmax temperature\n",
    "        self.eta = ..\n",
    "        \n",
    "    def train(self, observation, action, reward, done):\n",
    "        if reward is not None and action is not None:\n",
    "            self.product_rewards[action] += reward\n",
    "            self.product_counts[action] += 1\n",
    "            \n",
    "    def log_softmax(self, vec):\n",
    "        return vec - logsumexp(vec)\n",
    "\n",
    "    def softmax(self, vec):\n",
    "        probs = np.exp(self.log_softmax(vec))\n",
    "        probs /= probs.sum()\n",
    "        return probs\n",
    "    \n",
    "    def act(self, observation):\n",
    "        # TODO: compute probability of choosing action using Boltzmann exploration\n",
    "        prob = ..\n",
    "        # TODO: sample an action\n",
    "        action = ..\n",
    "\n",
    "        return { 'a': action, 'ps': prob[action] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare UCB and Exp3 agents performance and running time in stochastic bandits setting\n",
    "\n",
    "* Train and evaluate UCB and Exp3 algorithms against random product recommendation\n",
    "\n",
    "* Increase the number of products to 100 and explain the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of products to recommend\n",
    "num_products = 10\n",
    "# number of users for train and evaluation\n",
    "num_train_users, num_eval_users = 1000, 1000\n",
    "\n",
    "custom_args = { 'num_products': num_products,\n",
    "                'random_seed': RND_SEED,\n",
    "              }\n",
    "config = Configuration({ \n",
    "        **env_1_args,\n",
    "        **custom_args,\n",
    "})\n",
    "\n",
    "ucb_agent = AlphaUCBAgent(config)\n",
    "exp3_agent = Exp3Agent(config)\n",
    "rand_agent = RandomAgent(config)\n",
    "\n",
    "agents = [rand_agent, ucb_agent, exp3_agent]\n",
    "\n",
    "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
    "print(stats)\n",
    "\n",
    "plot_ctr(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare UCB and Exp3 agents in a non-stationary setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the simulation to have some updates in the preference of the users thought they are initialized equally (using the random_seed given in the custom_args). This modelization of the user state change leads to non-stationary user response to a recommendation and is dependant of what you recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_products = 100\n",
    "num_train_users, num_eval_users = 1000, 1000\n",
    "\n",
    "# adversarial setting: increase user state change during time\n",
    "custom_args = { 'num_products': num_products,\n",
    "                'random_seed': RND_SEED,                \n",
    "                'sigma_omega': 0.3\n",
    "              }\n",
    "config = Configuration({ \n",
    "        **env_1_args,\n",
    "        **custom_args,\n",
    "})\n",
    "\n",
    "ucb_agent = AlphaUCBAgent(config)\n",
    "exp3_agent = Exp3Agent(config)\n",
    "rand_agent = RandomAgent(config)\n",
    "\n",
    "agents = [rand_agent, ucb_agent, exp3_agent]\n",
    "\n",
    "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
    "print(stats)\n",
    "\n",
    "plot_ctr(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sample complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$-subgaussian distribution\n",
    "* UCB\n",
    "$R_T = \\mathcal{O}(\\sum_{i>1} \\frac{\\log T}{\\Delta_i})$\n",
    "* Exp3\n",
    "$R_T = \\mathcal{O}(\\sum_{i>1}\\frac{\\log^2 (T \\Delta_i^2)}{\\Delta_i})$\n",
    "\n",
    "distribution-independent\n",
    "* UCB\n",
    "$R_T = \\mathcal{O}(\\sqrt{KT\\log T})$\n",
    "* Exp3\n",
    "$R_T = \\mathcal{O}(\\sqrt{KT}\\log K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using side data: browsing events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make algorithms more sample efficient, we will use side data to bootstrap the recommendation. Specifically, we will use the user browsing events (aka \"organic\" events). The amount of browsing data is typically much larger than the number of click events on recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpliest recommedation agent is based on the total number of views of the product during browsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PopularityAgent(Agent):\n",
    "    def __init__(self, config):\n",
    "        super(PopularityAgent, self).__init__(config)\n",
    "\n",
    "        # Track number of times each item is viewed during browsing\n",
    "        self.nb_views = np.ones(self.config.num_products)\n",
    "\n",
    "    def train(self, observation, action, reward, done):\n",
    "        if observation:\n",
    "            for view in observation:\n",
    "                # TODO: code the update for nb_views that increments view counts\n",
    "                ..\n",
    "\n",
    "    def act(self, observation):\n",
    "        # TODO: compute probability of choosing an action proportionally to the total number of views\n",
    "        prob = ..\n",
    "        # TODO: choose action\n",
    "        action = ..\n",
    "        \n",
    "        return { 'a': action, 'ps': prob[action] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Bandit\n",
    "\n",
    "Improve the popularity agent by personalizing popularity to the user interest. We represent the user interest by the last product he/she has seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "class ContextualExp3Agent(Agent):\n",
    "    def __init__(self, config):\n",
    "        super(ContextualExp3Agent, self).__init__(config)\n",
    "\n",
    "        self.product_rewards = np.zeros((self.config.num_products, self.config.num_products))\n",
    "        self.last_product_seen = None\n",
    "        # softmax temperature parameter\n",
    "        self.eta = 0.01\n",
    "                \n",
    "    def update_lps(self, observation):\n",
    "        \"\"\"Update the last product seen based on the current observation\"\"\"\n",
    "        if observation:\n",
    "            self.last_product_seen = observation[-1]\n",
    "            \n",
    "    def train(self, observation, action, reward, done):\n",
    "        if observation:\n",
    "            # TODO: code the update for product_rewards matrix\n",
    "            ..\n",
    "            \n",
    "    def log_softmax(self, vec):\n",
    "        return vec - logsumexp(vec)\n",
    "\n",
    "    def softmax(self, vec):\n",
    "        probs = np.exp(self.log_softmax(vec))\n",
    "        probs /= probs.sum()\n",
    "        return probs\n",
    "    \n",
    "    def act(self, observation):\n",
    "        self.update_lps(observation)\n",
    "        \n",
    "        # TODO: compute probability of choosing an action given last seen product by the user\n",
    "        prob = ..\n",
    "        # TODO: choose action\n",
    "        action = ..\n",
    "\n",
    "        return { 'a': action, 'ps': prob[action] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare agents that use click events to agents that use view events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_products = 100\n",
    "num_train_users, num_eval_users = 1000, 1000\n",
    "\n",
    "# increase difference among users\n",
    "custom_args = { 'num_products': num_products,\n",
    "                'random_seed': RND_SEED,\n",
    "                'sigma_omega_initial': 2.0,                \n",
    "              }\n",
    "config = Configuration({ \n",
    "        **env_1_args,\n",
    "        **custom_args,\n",
    "})\n",
    "\n",
    "contextual_exp3_agent = ContextualExp3Agent(config)\n",
    "exp3_agent = Exp3Agent(config)\n",
    "pop_agent = PopularityAgent(config)\n",
    "ucb_agent = AlphaUCBAgent(config)\n",
    "rand_agent = RandomAgent(config)\n",
    "\n",
    "agents = [pop_agent, contextual_exp3_agent, exp3_agent, ucb_agent, rand_agent]\n",
    "\n",
    "stats = train_eval_agents(agents, config, num_train_users, num_eval_users)\n",
    "print(stats)\n",
    "\n",
    "plot_ctr(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline evaluation\n",
    "\n",
    "Running counterfactual evaluations on real datasets use more time than a reasonnable practical sessions does. \n",
    "We are going to exhibit the core of the problems theses algorithms faces on a very toy example. \n",
    "\n",
    "Assume we are running a $\\pi_0$ policy random uniform on the possible sets of arms of size 20 with a number of context 10. This means that at each timestep we have 200 possibles combinations to create the recommendation which is very small. Moreover we assume that the obtained reward is a constant equal to 1. This means we have no stochasticity due to realization of the sample and any policy $\\pi$ should have the same estimator of averaged reward.\n",
    "\n",
    "1. We assume the new $\\pi$ is always choosing arm 1, plot the estimation of the IPS  with respect to the number of samples on one run. If you want to be close to the reality (and do more sophisticated experiments), it is better to simulate a dataset $S=\\{context_t, action_t, propensity_t, reward_t\\}$ and then write the IPS estimation rather than rely on mathematical properties of our minimalist system. Also remark than in real system this is a good idea to also store the possible set of actions at each timestep. What do you think of this estimator while you can sample $S$ only one time ? \n",
    "\n",
    "2. Supperpose the self normalized operator on the same plot for each time step. Keep in mind this is actually the best possible case for SNIPS and that for more complex scenarios many others problems can be more important than the variance fixed by snips. \n",
    "\n",
    "3. Now the reward of arm 2 is 1 with probability 1/2 and 1 with probability 1/3 for all other arms. The $\\pi_0$ policy select arm one with probability 0.9 and an arm at random with proba 0.1. Build several estimates of the policy selecting always arm 2. How many samples do you need to identify with confidence that this policy is better than the logging policy ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RecoGym https://github.com/criteo-research/reco-gym\n",
    "* Bandit Algorithms, 2018 by Tor Lattimore and Csaba Szepesv√°ri"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
