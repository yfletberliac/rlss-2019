{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run this cell to set your notebook up (only mandatory if rlss2019-docker image is not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "!git clone https://github.com/yfletberliac/rlss2019-hands-on.git > /dev/null 2>&1\n",
    "!pip install -q torch==1.1.0 torchvision pyvirtualdisplay piglet > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#ed7d31'>Deep Q Networks</font>\n",
    "------------\n",
    "You can find the original paper [here](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Preliminaries: Q Learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#ed7d31'>Q-Value</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Value** is a measure of the overall expected reward assuming the agent is in state $s$ and performs action $a$, and then continues playing until the end of the episode following some policy $\\pi$. It is defined mathematically as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}\\left(s_{t}, a_{t}\\right)=E\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots | s_{t}, a_{t}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $R_{t+1}$ is the immediate reward received after performing action $a_{t}$ in state $s_{t}$ and $\\gamma$ is the discount factor and controls the importance of the future rewards versus the immediate ones: the lower the discount factor is, the less important future rewards are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#ed7d31'>Bellman Optimality Equation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the Bellman equation defines the relationships between a given state (or, in our case, a **state-action pair**) and its successors. While many forms exist, one of the most common is the **Bellman Optimality Equation** for the optimal **Q-Value**, which is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{*}(s, a)=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Of course, when no uncertainty exists (transition probabilities are either 0 or 1), we have:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{*}(s, a)=r(s, a)+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#ed7d31'>Q-Value Iteration</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the corresponding Bellman backup operator:\n",
    "\\begin{equation}\n",
    "[\\mathcal{T} Q]\\left(s, a\\right)=r(s, a)+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$ is a fixed point of $\\mathcal{T}$:\n",
    "\\begin{equation}\n",
    "\\mathcal{T} Q^{*}=Q^{*}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the Bellman operator $\\mathcal{T}$ repeatedly to any initial $Q$, the series converges to $Q^{*}$:\n",
    "\\begin{equation}\n",
    "Q, \\mathcal{T} Q, \\mathcal{T}^{2} Q, \\cdots \\rightarrow Q^{*}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#ed7d31'>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './rlss2019-hands-on/utils')\n",
    "# If using the Docker image, replace by:\n",
    "# sys.path.insert(0, '../utils')\n",
    "\n",
    "import gym, random, os.path, math, glob, csv, base64\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from qfettes_plot import plot_all_data\n",
    "from qfettes_wrappers import *\n",
    "from openai_wrappers import make_atari, wrap_deepmind\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#ed7d31'>Deep Q learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in Deep RL, the **Q-Value** is defined as $Q(s,a;\\theta)$ where $\\theta$ represents the parameters of the function approximation used.\n",
    "\n",
    "<img src=\"../imgs/approx.png\" alt=\"Drawing\" width=\"200\"/>\n",
    "\n",
    "For *MuJoCo* or *Roboschool* environments, we usually use a simple 2- or 3-layer MLP whereas when using **raw pixels for observations** such as in *Atari 2600* games, we usually use a 1-, 2- or 3-layer CNN.\n",
    "\n",
    "In our case, since we want to train DQN on *CartPole*, we will use a 3-layer perceptron for our function approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Network declaration</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build $Q(s,a;\\theta)$ function approximation. Since the input is composed of 4 scalars, namely:\n",
    "<center>[position of cart, velocity of cart, angle of pole, rotation rate of pole]</center>\n",
    "we build a FCN -> ReLU -> FCN -> ReLU -> FCN neural network. As an exercice, change the architecture of the network:\n",
    "\n",
    "1. Change the two fully-connected layers from 8 hidden neurons to 16\n",
    "3. Add a third layer to the network, with no activation function and `self.num_actions` as the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_shape[0], 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.fc3 = nn.Linear(8, self.num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Safety checks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#ed7d31'>Network architecture</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a *safety check*, inspect the resulting network in the next cell. For instance, the total number of trainable parameters should change with the architecture. Check the correctness of `in_features` and `out_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "env    = gym.make(env_id)\n",
    "network = DQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "print(\"Observation space:\\n\", env.observation_space.shape, \"\\n\")\n",
    "print(\"Network architecture:\\n\", network, \"\\n\")\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, network.parameters())\n",
    "print(\"Total number of trainable parameters:\\n\", sum([np.prod(p.size()) for p in model_parameters]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#ed7d31'>Run a Policy with Random Actions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the working environment looks like? It's always useful to know the details about the environment you train your policy on. For instance, its dynamics, the size of action and observation space, etc. Below we display three different random policies on `CartPole-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(2):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the episode ending prematurely because the pole drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<font color='#ed7d31'>**Question**:</font>\n",
    "\n",
    "It is also important to identify some of the characteristics of the problem. `CartPole-v0` can be described as a **fully-observable**, **deterministic**, **continuous state space**, with a **discrete action space** and **frequent rewards**. Take some time to understand each of these terms :-) Try to find the opposite term for each of them, e.g. deterministic <> stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Experience Replay Memory</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual RL tasks have no pre-generated training sets which they can learn from, in off-policy learning, our agent must keep records of all the state-transitions it encountered so it can **learn from them later**. The memory-buffer used to store this is often referred to as the **Experience Replay Memory**. There are several types and architectures of these memory buffers — but some very common ones are:\n",
    "- the *cyclic memory buffers*: they make sure the agent keeps training over its new behavior rather than things that might no longer be relevant\n",
    "- the *reservoir-sampling-based memory buffers*: they guarantee each state-transition recorded has an even probability to be inserted to the buffer\n",
    "\n",
    "We use a combination of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        # This function needs an `if` statement in order to keep the capacity to its limit. Write it below.\n",
    "        # Hint: `del something` will delete something if something is an array\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have:\n",
    "- the **DQN** network,\n",
    "- the **ExperienceReplayMemory**.\n",
    "\n",
    "Let's build the **Agent** class !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Agent declaration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, config, env, log_dir='/tmp/gym'):\n",
    "        self.log_dir = log_dir\n",
    "        self.rewards = []\n",
    "        self.action_log_frequency = config.ACTION_SELECTION_COUNT_FREQUENCY\n",
    "        self.action_selections = [0 for _ in range(env.action_space.n)]\n",
    "    \n",
    "    # Define the DQN networks\n",
    "    def declare_networks(self):\n",
    "        self.model = DQN(self.num_feats, self.num_actions)\n",
    "        # Create `self.target_model` with the same network architecture\n",
    "        self.target_model = DQN(self.num_feats, self.num_actions)\n",
    "\n",
    "    # Define the Replay Memory\n",
    "    def declare_memory(self):\n",
    "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "    \n",
    "    # Append the new transition to the Replay Memory\n",
    "    def append_to_replay(self, s, a, r, s_):\n",
    "        self.memory.push((s, a, r, s_))\n",
    "    \n",
    "    # Sample transitions from the Replay Memory\n",
    "    def sample_minibatch(self):\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (-1,)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        # Sometimes all next states are false\n",
    "        try:\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            non_final_next_states = None\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values\n",
    "    \n",
    "    # Sample action\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            # Epsilon-greedy\n",
    "            if np.random.random() >= eps:\n",
    "                X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
    "                a = self.model(X).max(1)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<font color='#ed7d31'>**Question**:</font>\n",
    "\n",
    "Remember we define the objective function as\n",
    "\\begin{equation}\n",
    "J=\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}, \\mathbf{\\theta}^{-}\\right)-Q(s, a, \\mathbf{\\theta})\\right)^{2},\n",
    "\\end{equation}\n",
    "where $\\theta^{-}$ are the target parameters.\n",
    "\n",
    "Why do we need a target network in the first place ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Learning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning(Agent):\n",
    "    def __init__(self, env=None, config=None, log_dir='/tmp/gym'):\n",
    "        super().__init__(config=config, env=env, log_dir=log_dir)\n",
    "    \n",
    "    # Compute loss from the Bellman Optimality Equation\n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values = batch_vars\n",
    "\n",
    "        # Estimate\n",
    "        current_q_values = self.model(batch_state).gather(1, batch_action)\n",
    "        \n",
    "        # Target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            if not empty_next_state_values:\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
    "        # From the equation above, write the value `expected_q_values`.\n",
    "            expected_q_values = batch_reward + self.gamma*max_next_q_values\n",
    "        \n",
    "        # From the equation above, write the value `diff`.\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.MSE(diff)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Update both networks (the agent and the target)\n",
    "    def update(self, s, a, r, s_, sample_idx=0):\n",
    "        self.append_to_replay(s, a, r, s_)\n",
    "        \n",
    "        # When not to update ?\n",
    "        # There is a concise way to write to skip the update, fill in the 2 blanks in the `if` statement below.\n",
    "        # Hint: the sample count should be < the learn_start hyperparameter and respect the update_freq.\n",
    "        # if ... or ...:\n",
    "        if sample_idx < self.learn_start or sample_idx % self.update_freq != 0:\n",
    "            return None\n",
    "\n",
    "        batch_vars = self.sample_minibatch()\n",
    "        loss = self.compute_loss(batch_vars)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_model()\n",
    "        self.save_td(loss.item(), sample_idx)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Copy weights from model to target_model following `target_net_update_freq`.\n",
    "        self.update_count+=1\n",
    "        if self.update_count % self.target_net_update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Model declaration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Learning):\n",
    "    def __init__(self, env=None, config=None, log_dir='/tmp/gym'):\n",
    "        super().__init__(config=config, env=env, log_dir=log_dir)\n",
    "        self.device = config.device\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = config.GAMMA\n",
    "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n",
    "        self.experience_replay_size = config.EXP_REPLAY_SIZE\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.learn_start = config.LEARN_START\n",
    "        self.update_freq = config.UPDATE_FREQ\n",
    "\n",
    "        # Environment specific parameters\n",
    "        self.num_feats = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.declare_networks()\n",
    "        self.declare_memory()\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LR)\n",
    "        \n",
    "        # Move to correct device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        self.target_model.train()\n",
    "        \n",
    "        self.update_count = 0\n",
    "            \n",
    "    def save_td(self, td, tstep):\n",
    "        with open(os.path.join(self.log_dir, 'td.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow((tstep, td))\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\n",
    "    \n",
    "    def MSE(self, x):\n",
    "        return 0.5 * x.pow(2)\n",
    "\n",
    "    def save_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def save_action(self, action, tstep):\n",
    "        self.action_selections[int(action)] += 1.0/self.action_log_frequency\n",
    "        if (tstep+1) % self.action_log_frequency == 0:\n",
    "            with open(os.path.join(self.log_dir, 'action_log.csv'), 'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(list([tstep]+self.action_selections))\n",
    "            self.action_selections = [0 for _ in range(len(self.action_selections))]\n",
    "            \n",
    "    def save_w(self):\n",
    "        if not os.path.exists(\"../saved_agents\"):\n",
    "            os.makedirs(\"../saved_agents\")\n",
    "        torch.save(self.model.state_dict(), '../saved_agents/model.dump')\n",
    "        torch.save(self.optimizer.state_dict(), '../saved_agents/optim.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Main agent variables\n",
    "        self.GAMMA=0.99\n",
    "        self.LR=1e-3\n",
    "        \n",
    "        # Epsilon variables\n",
    "        self.epsilon_start    = 1.0\n",
    "        self.epsilon_final    = 0.01\n",
    "        self.epsilon_decay    = 10000\n",
    "        self.epsilon_by_sample = lambda sample_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * sample_idx / config.epsilon_decay)\n",
    "\n",
    "        # Memory\n",
    "        self.TARGET_NET_UPDATE_FREQ = 1000\n",
    "        self.EXP_REPLAY_SIZE = 10000\n",
    "        self.BATCH_SIZE = 64\n",
    "\n",
    "        # Learning control variables\n",
    "        self.LEARN_START = 1000\n",
    "        self.MAX_SAMPLES = 50000\n",
    "        self.UPDATE_FREQ = 1\n",
    "\n",
    "        # Data logging parameters\n",
    "        self.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n",
    "        \n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from openai_monitor import Monitor\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "start=timer()\n",
    "\n",
    "log_dir = \"/tmp/gym/\"\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*td.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*action_log.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "env_id = 'CartPole-v0'\n",
    "env    = gym.make(env_id)\n",
    "env    = Monitor(env, os.path.join(log_dir, env_id))\n",
    "        \n",
    "model  = Model(env=env, config=config, log_dir=log_dir)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "for sample_idx in range(1, config.MAX_SAMPLES + 1):\n",
    "    \n",
    "    epsilon = config.epsilon_by_sample(sample_idx)\n",
    "\n",
    "    action = model.get_action(observation, epsilon)\n",
    "    # Log action selection\n",
    "    model.save_action(action, sample_idx)\n",
    "\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    model.update(prev_observation, action, reward, observation, sample_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "    if sample_idx % 1000 == 0:\n",
    "        try:\n",
    "            clear_output(True)\n",
    "            plot_all_data(log_dir, env_id, 'DQN', config.MAX_SAMPLES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n",
    "        except IOError:\n",
    "            pass\n",
    "\n",
    "model.save_w()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the plots, does the learning appear to be stable?\n",
    "\n",
    "If your answer is *yes*, then start a second run, and a thirs, with the same hyperparameters. ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#ed7d31'>Visualize the agent</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Loading the agent\n",
    "fname_model = \"../saved_agents/model.dump\"\n",
    "fname_optim = \"../saved_agents/optim.dump\"\n",
    "log_dir = \"/tmp/gym/\"\n",
    "\n",
    "model  = Model(env=env, config=config, log_dir=log_dir)\n",
    "\n",
    "if os.path.isfile(fname_model):\n",
    "    model.model.load_state_dict(torch.load(fname_model))\n",
    "    model.target_model.load_state_dict(model.model.state_dict())\n",
    "\n",
    "if os.path.isfile(fname_optim):\n",
    "    model.optimizer.load_state_dict(torch.load(fname_optim))\n",
    "\n",
    "env_id = 'CartPole-v0'\n",
    "env    = gym.make(env_id)\n",
    "env    = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(3):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = model.get_action(obs)\n",
    "        obs, _, done, _ = env.step(action)\n",
    "\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with modifying the hypermarameters (learning rate, batch size, experience replay size, etc.) to see if you can make its behaviour improve !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
